# Backend de IA "Bare-Metal" (No-Framework)

> **"Inferencia de IA sin el peso de Docker ni Python en runtime."**

![IA Demo](demo.gif)

## â“ El Problema Real
Desplegar modelos de IA en producciÃ³n suele ser ineficiente.
*   **Python** es lento para servir peticiones HTTP concurrentes (GIL).
*   **TensorFlow/PyTorch** son librerÃ­as gigantescas (>500MB) difÃ­ciles de desplegar en entornos ligeros.

## ğŸ›  La SoluciÃ³n ArquitectÃ³nica
Un servidor de inferencia desde cero que elimina la dependencia de Python en producciÃ³n:

1.  **Python (Solo Entrenamiento)**: Se usa para diseÃ±ar y entrenar la red neuronal. Exporta los pesos a un formato binario simple (`weights.bin`).
2.  **Java (API Gateway)**: Recibe las peticiones REST y gestiona la cola de trabajos.
3.  **Rust (Motor de Inferencia)**: Carga `weights.bin` y ejecuta la multiplicaciÃ³n de matrices usando instrucciones vectoriales (SIMD) de la CPU.

### Â¿Por quÃ© no usar TorchServe?
Esta implementaciÃ³n demuestra cÃ³mo construir un motor de inferencia personalizado para sistemas embebidos o de latencia crÃ­tica donde no puedes permitirte el overhead de un framework completo.

## âš™ï¸ CÃ³mo Ejecutar
El script `manage.py` orquesta el entrenamiento (si es necesario) y la ejecuciÃ³n:

```bash
python ../manage.py run ia
```

## ğŸ“ˆ Escalabilidad
Al desacoplar el servidor HTTP (Java) del cÃ³mputo (Rust), podemos ajustar el nÃºmero de hilos de inferencia independientemente de las conexiones de red, maximizando el uso de la CPU.
