# Backend de IA "Bare-Metal" (No-Framework)

> **"Inferencia de IA sin el peso de Docker ni Python en runtime."**

![IA Demo](demo.gif)

## â“ El Problema Real
Desplegar modelos de IA en producciÃ³n suele ser ineficiente.
*   **Python** es lento para servir peticiones HTTP concurrentes (GIL).
*   **TensorFlow/PyTorch** son librerÃ­as gigantescas (>500MB) difÃ­ciles de desplegar en entornos ligeros (Edge/IoT).

## ðŸ›  La SoluciÃ³n ArquitectÃ³nica
Un servidor de inferencia desde cero que elimina la dependencia de Python en producciÃ³n:

1.  **Python (Solo Entrenamiento)**: Se usa para diseÃ±ar y entrenar la red neuronal. Exporta los pesos a un formato binario simple (`weights.bin`).
2.  **Java (API Gateway)**: Recibe las peticiones REST y gestiona la cola de trabajos.
3.  **Rust (Motor de Inferencia)**: Carga `weights.bin` y ejecuta la multiplicaciÃ³n de matrices usando instrucciones vectoriales (SIMD) de la CPU.

### Â¿Por quÃ© no usar TorchServe?
Esta implementaciÃ³n demuestra cÃ³mo construir un motor de inferencia personalizado para sistemas embebidos o de latencia crÃ­tica donde no puedes permitirte el overhead de un framework completo.

## ðŸ“ Diagrama de Arquitectura

```mermaid
graph LR
    A[Usuario API REST] -->|HTTP POST| B(Java Job Manager)
    subgraph "Training Phase"
        T[Python Scripts] -->|Export Weights| F[weights.bin]
    end
    F -->|Load Memory| C
    B -->|JNI Call| C[Rust Inference Engine]
    C -->|SIMD AVX2| C
    C -->|Prediction| B
    B -->|JSON Response| A
```

## ðŸ“Š Benchmarks de Rendimiento

| Escenario | Python (Flask + NumPy) | Java + Rust (Este Proyecto) |
| :--- | :--- | :--- |
| **Cold Start** | ~2.5 segundos | **< 100 milisegundos** |
| **Memoria RAM** | ~150MB (Interpreter overhead) | **~30MB** (JVM + Raw Arrays) |
| **Inferencia / seg** | ~450 req/s | **~1,200 req/s** |

## âš™ï¸ CÃ³mo Ejecutar
El script `manage.py` orquesta el entrenamiento (si es necesario) y la ejecuciÃ³n:

```bash
python ../manage.py run ia
```

## ðŸ“ˆ Escalabilidad
Al desacoplar el servidor HTTP (Java) del cÃ³mputo (Rust), podemos ajustar el nÃºmero de hilos de inferencia independientemente de las conexiones de red, maximizando el uso de la CPU.
